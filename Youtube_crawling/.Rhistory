top_n(10)
str(stop_words)
custom = data.frame(word = c("https", "t.co", "amp", "rt"),
lexicon = c("SMART", "SMART", "SMART", "SMART"))
tidy_review <- tidy_review %>%
anti_join(stop_words)
# Remove custom words
tidy_review <- tidy_review %>%
anti_join(custom)
tidy_review %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(10)
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
head(trumptweets)
str(trumptweets)
dim(trumptweets)
names(trumptweets)
trump_text <- trumptweets[,"text"]
str(trump_text)
trump_text <- gsub("\n", "", trump_text)
head(trump_text)
head(trump_text)
trump_text <- trumptweets[,"text"]
head(trump_text)
trump_text <- gsub("\n", "", trump_text)
head(trump_text)
#install.packages("dplyr")
#install.packages("rvest")
library(rvest)
library(dplyr)
#### Text analysis ####
ironurl <- "https://www.metacritic.com/movie/iron-man/user-reviews?page="
score_all <- NULL
review_all <- NULL
author_all <- NULL
for (i in 0:5) {
ironurl_tmp <- paste0(ironurl, i)
score <- read_html(ironurl_tmp) %>% html_nodes(".indiv") %>% html_text()
score_all <- c(score_all, score)
review <- read_html(ironurl_tmp) %>% html_nodes(".review_body") %>% html_text()
review_all <- c(review_all, review)
author <- read_html(ironurl_tmp) %>% html_nodes(".author") %>% html_text()
author_all <- c(author_all, author)
}
# WEB SCRAPE IS DIRTY
# Let's see why
length(score_all)
length(review_all)
length(author_all)
# Let's check why it happens
head(score_all, 3)
head(review_all, 3)
head(author_all, 3)
# Let's delete empty spaces in review_all
review_all <- review_all[review_all!=""] # data dirtiness from web scrape
head(review_all, 3)
str(review_all)
trump_text <- trumptweets[,"text"]
head(trump_text)
#trump_text <- gsub("\n", "", trump_text)
head(trump_text)
library(rvest)
library(dplyr)
review <- data.frame(time = trumptweets[,"created_at"], text = trumptweets[,"text"])
head(review)
library(tidytext)
tidy_review <- review %>%
unnest_tokens(word, text)
head(tidy_review)
tidy_review %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(10)
custom = data.frame(word = c("https", "t.co", "amp", "rt"),
lexicon = c("SMART", "SMART", "SMART", "SMART"))
tidy_review <- tidy_review %>%
anti_join(stop_words)
# Remove custom words
tidy_review <- tidy_review %>%
anti_join(custom)
tidy_review %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(10)
# From skeleton code
tidy_review <- tidy_review[-grep("\\b\\d+\\b", tidy_review$word),]
isTRUE("css "=="css")
tidy_review %>%
count(word) %>%
arrange(desc(n))
library(SnowballC)
tidy_review <- tidy_review %>%
mutate_at("word", ~wordStem(., language="en"))
tidy_review %>%
count(word) %>%
arrange(desc(n))
head(tidy_review)
head(tidy_review, n=100)
View(trumptweets)
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptime <- trumptweets$created_at
trumptext <- trumptweets$text
library(rvest)
library(dplyr)
library(tidytext)
# remove tweet links
trumptextcull <- gsub("(https://t.co/)(.*)","",trumptext)
# remove retweet tags
trumptextcull <- gsub("(RT @)(.*)[:]","",trumptextcull)
# remove mentions
trumptextcull <- gsub("(@)(.*)\\s","",trumptextcull)
# remove hashtags
trumptextcull <- gsub("(#)(.*)\\s","",trumptextcull)
# remove formatting artifacts
trumptextcull <- gsub("(\n)","",trumptextcull)
# removing punctuation
trumptextcull <- gsub("[[:punct:]]","",trumptextcull)
# fixing "amp" to "and" typo
trumptextcull <- gsub("amp","and",trumptextcull)
# indexes of blank and 1-character long tweets to be culled
cullindex <- which(nchar(trumptextcull) %in% c(0,1))
trumptextcull <- trumptextcull[-cullindex]
trumptime <- trumptime[-cullindex]
head(trumptextcull)
# unnesting tweets into individual words per timestamp
tidy_review <- review %>%
unnest_tokens(word,text) %>%
rev(.)
head(tidy_review)
likes <- trumptweets$favorite_count
retweet <- trumptweets$retweet_count
likes <- likes[-cullindex]
retweet <- retweet[-cullindex]
dim(likes)
dim(retweet)
likes
retweet
str(retweet)
dim(retweet)
dim(trumptweets)
review2 <- data.frame(like = likes, ret = retweet, text = trumptextcull)
View(review2)
review2 <- data.frame(like = likes, ret = retweet, text = trumptextcull)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
View(tidy_review2)
test_str <- c(unnest_tokens("I like you"))
test_str <- unnest_tokens("I like you")
test_str <- unnest_tokens(word, "I like you")
review2 <- data.frame(time = trumptime, like = likes, ret = retweet, text = trumptextcull)
View(review2)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
View(tidy_review2)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments)
View(tidy_review_afinn2)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
View(tidy_review2)
View(tidy_review2)
View(tidy_review_afinn2)
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptime <- trumptweets$created_at
trumptext <- trumptweets$text
library(rvest)
library(dplyr)
library(tidytext)
# remove tweet links
trumptextcull <- gsub("(https://t.co/)(.*)","",trumptext)
# remove retweet tags
trumptextcull <- gsub("(RT @)(.*)[:]","",trumptextcull)
# remove mentions
trumptextcull <- gsub("(@)(.*)\\s","",trumptextcull)
# remove hashtags
trumptextcull <- gsub("(#)(.*)\\s","",trumptextcull)
# remove formatting artifacts
trumptextcull <- gsub("(\n)","",trumptextcull)
# removing punctuation
trumptextcull <- gsub("[[:punct:]]","",trumptextcull)
# fixing "amp" to "and" typo
trumptextcull <- gsub("amp","and",trumptextcull)
# indexes of blank and 1-character long tweets to be culled
cullindex <- which(nchar(trumptextcull) %in% c(0,1))
trumptextcull <- trumptextcull[-cullindex]
trumptime <- trumptime[-cullindex]
# reformatting the time to YYYY-MM-DD
trumptime <- gsub("\\s(.*)","",trumptime)
# cleaning done
# creating a table combining the timestamps and the cleaned tweets
review <- data.frame(time = trumptime, text = trumptextcull)
# unnesting tweets into individual words per timestamp
tidy_review <- review %>%
unnest_tokens(word,text)
# removing stop words
tidy_review <- tidy_review %>%
anti_join(stop_words)
# check the data
tidy_review %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(10)
library(SnowballC)
# stemming the list of words
tidy_review <- tidy_review %>%
mutate_at("word", ~wordStem(., language="en"))
library(textdata)
# create a new lexicon that eliminates "trump" to avoid positivity-bias
sentiments <- get_sentiments("afinn") %>% filter(!word %in% "trump")
# get the average sentiment of each individual timestamped tweet
tidy_review_afinn <- tidy_review %>%
inner_join(sentiments) %>%
group_by(time) %>%
summarise(mean_feeling = mean(value))
# plot
ggplot(tidy_review_afinn, aes(time,mean_feeling, group=1)) + geom_line()
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptime <- trumptweets$created_at
trumptext <- trumptweets$text
library(rvest)
library(dplyr)
library(tidytext)
# remove tweet links
trumptextcull <- gsub("(https://t.co/)(.*)","",trumptext)
# remove retweet tags
trumptextcull <- gsub("(RT @)(.*)[:]","",trumptextcull)
# remove mentions
trumptextcull <- gsub("(@)(.*)\\s","",trumptextcull)
# remove hashtags
trumptextcull <- gsub("(#)(.*)\\s","",trumptextcull)
# remove formatting artifacts
trumptextcull <- gsub("(\n)","",trumptextcull)
# removing punctuation
trumptextcull <- gsub("[[:punct:]]","",trumptextcull)
# fixing "amp" to "and" typo
trumptextcull <- gsub("amp","and",trumptextcull)
# indexes of blank and 1-character long tweets to be culled
cullindex <- which(nchar(trumptextcull) %in% c(0,1))
trumptextcull <- trumptextcull[-cullindex]
trumptime <- trumptime[-cullindex]
# reformatting the time to YYYY-MM-DD
trumptime <- gsub("\\s(.*)","",trumptime)
# cleaning done
# creating a table combining the timestamps and the cleaned tweets
review <- data.frame(time = trumptime, text = trumptextcull)
# unnesting tweets into individual words per timestamp
tidy_review <- review %>%
unnest_tokens(word,text)
# removing stop words
tidy_review <- tidy_review %>%
anti_join(stop_words)
# check the data
tidy_review %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(10)
library(SnowballC)
# stemming the list of words
tidy_review <- tidy_review %>%
mutate_at("word", ~wordStem(., language="en"))
library(textdata)
library(ggplot2)
# create a new lexicon that eliminates "trump" to avoid positivity-bias
sentiments <- get_sentiments("afinn") %>% filter(!word %in% "trump")
# get the average sentiment of each individual timestamped tweet
tidy_review_afinn <- tidy_review %>%
inner_join(sentiments) %>%
group_by(time) %>%
summarise(mean_feeling = mean(value))
# plot
ggplot(tidy_review_afinn, aes(time,mean_feeling, group=1)) + geom_line()
# ignore these lines below vvvvvvvvvvvvv
# get the day that a tweet was made on
timestamp_day <- gsub("^(.*)[-]","",trumptime)
# get the month that a tweet was made on
timestamp_month <- substring(trumptime,6)
timestamp_month <- gsub(".{3}$","",timestamp_month)
likes <- trumptweets$favorite_count
retweet <- trumptweets$retweet_count
likes <- likes[-cullindex]
retweet <- retweet[-cullindex]
likes
retweet
review2 <- data.frame(time = trumptime, like = likes, ret = retweet, text = trumptextcull)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments)
View(sentiments)
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(time)
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
summarise(mean_feeling = mean(value))
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(time) %>%
summarise(mean_feeling = mean(value))
View(tidy_review_afinn2)
View(tidy_review2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(time) %>%
summarise(mean_feeling = mean(value), likes = mean(like), retweets = mean(ret))
View(tidy_review_afinn2)
review2 <- data.frame(time = trumptime, like = likes, ret = retweet, text = trumptextcull, row = )
review2$index <- rownames(review2)
review2 <- data.frame(time = trumptime, like = likes, ret = retweet, text = trumptextcull)
review2$index <- rownames(review2)
View(review2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(index) %>%
summarise(mean_feeling = value, likes = mean(like), retweets = mean(ret))
View(tidy_review_afinn2)
review2 <- data.frame(time = trumptime, like = likes, ret = retweet, text = trumptextcull)
review2$index <- rownames(review2)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(index) %>%
summarise(mean_feeling = value, likes = mean(like), retweets = mean(ret))
View(tidy_review_afinn2)
View(tidy_review2)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(index)
View(tidy_review_afinn2)
review2 <- data.frame(like = likes, ret = retweet, text = trumptextcull)
review2$index <- rownames(review2)
tidy_review2 <- review2 %>%
unnest_tokens(word,text)
tidy_review2 <- tidy_review2 %>%
anti_join(stop_words)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
group_by(index)
View(tidy_review_afinn2)
View(trumptweets)
review2 <- data.frame(time = trumptweets$create_at
, like = likes, ret = retweet, text = trumptextcull)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
subset(select = -c(:))
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
subset(select = -c(:))
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
subset(select = -word)
View(tidy_review_afinn2)
View(tidy_review2)
View(tidy_review2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments)# %>%
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
subset(select = -c(word)) # %>%
View(tidy_review_afinn2)
tidy_review_afinn2 <- tidy_review2 %>%
inner_join(sentiments) %>%
subset(select = -c(word)) %>%
group_by(index) %>%
summarize(likes = mean(like), retweet = mean(ret), value = mean(value))
View(tidy_review_afinn2)
library(tidyr)
library(dplyr)
library(reshape2)
library(lubridate)
getwd()
library(tidyr)
library(dplyr)
library(reshape2)
library(lubridate)
getwd()
setwd("~/Desktop/계산사회과학/FinalProject/Youtube_crawling")
setwd("~/Desktop/계산사회과학/FinalProject/Youtube_crawling")
setwd("~/Desktop/계산사회과학/FinalProject")
setwd("~/Desktop/계산사회과학/FinalProject/Youtube_crawling/")
setwd("~/Desktop/계산사회과학/FinalProject/Youtube_crawling")
getwd()
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
list.dir()
list.dirs()
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
text_neck <- read.csv("./data/거북목.csv")
getwd()
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
text_neck <- read.csv("./data/거북목.csv")
View(text_neck)
library(tidyr)
library(dplyr)
library(reshape2)
library(lubridate)
getwd()
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
text_neck <- read.csv("./data/거북목.csv")
round_shoulder <- read.csv("./data/라운드숄더.csv")
back_pain <- read.csv("./data/허리 통증.csv")
View(round_shoulder)
View(text_neck)
# Get rid of unnecessary part
remove_text_neck <- c("아는형님", "짐승친구들", "두턱", "병맛노래", "동물", "작살", "거북이", "짤툰", "Vlog", "TURTLE")
text_neck %>%
filter(!(X..word %in% remove_text_neck))
text_neck %>%
filter(!(title %in% remove_text_neck))
View(text_neck)
removed <- text_neck %>%
filter(!(title %in% remove_text_neck))
View(removed)
removed <- (text_neck %>%
filter(!(title %in% remove_text_neck)))
# Get rid of unnecessary part
remove_text_neck <- c("아는형님", "짐승친구들", "두턱", "병맛노래", "동물", "작살", "거북이", "짤툰", "Vlog", "TURTLE")
removed <- (text_neck %>%
filter(!(title %in% remove_text_neck)))
str(text_neck %>%
filter(!(title %in% remove_text_neck)))
text_neck %>%
filter(!(title %in% remove_text_neck))
str(text_neck %>%
filter(!(title %in% remove_text_neck)))
text_neck %>%
filter(!(title %in% remove_text_neck)) %>% str()
text_neck %>%
filter(!(title %in% remove_text_neck))
text_neck %>%
filter(title %in% remove_text_neck)
text_neck %>%
filter((title %in% remove_text_neck))
text_neck <- read.csv("./data/거북목.csv")
# Get rid of unnecessary part
remove_text_neck <- c("아는형님", "짐승친구들", "두턱", "병맛노래", "동물", "작살", "거북이", "짤툰", "Vlog", "TURTLE")
text_neck %>%
filter((title %in% remove_text_neck))
text_next$title %in% remove_text_neck
text_neck$title %in% remove_text_neck
df2 <- text_neck[!(row.names(text_neck) %in% remove_text_neck),]
View(df2)
row.names(text_neck)
column.names(text_neck)
View(back_pain)
text_neck %>%
filter(!(title %in% remove_text_neck))
str(text_neck %>%
filter(!(title %in% remove_text_neck)))
text_neck %>%
filter(!(title %in% remove_text_neck))
filter(text_neck, !(title %in% remove_text_neck))
removed <- filter(text_neck, !(title %in% remove_text_neck))
!(text_neck$title %in% remove_text_neck)
text_neck_token <- text_neck %>%
unnest_tokens(word, title)
library(tidytext)
text_neck_token <- text_neck %>%
unnest_tokens(word, title)
View(text_neck)
View(text_neck_token)
View(text_neck)
View(text_neck_token)
View(text_neck)
View(text_neck_token)
View(text_neck_token)
View(text_neck_token)
text_neck_token <- text_neck %>%
unnest_tokens(word, title) %>%
group_by(X)
View(text_neck_token)
View(text_neck)
View(text_neck_token)
text_neck %>%
unnest_tokens(word, title) %>%
group_by(X)
library(tidyr)
library(dplyr)
library(reshape2)
library(lubridate)
library(tidytext)
getwd()
setwd("~/Desktop/계산사회과학/FinalProject/youtube_crawling")
text_neck <- read.csv("./data/거북목.csv")
round_shoulder <- read.csv("./data/라운드숄더.csv")
back_pain <- read.csv("./data/허리 통증.csv")
# Get rid of unnecessary part
remove_text_neck <- c("아는형님", "짐승친구들", "두턱", "병맛노래", "동물", "작살", "거북이", "짤툰", "Vlog", "TURTLE")
text_neck %>%
unnest_tokens(word, title) %>%
group_by(X, add=TRUE)
removed <- filter(text_neck, !(title %in% remove_text_neck))
text_neck %>%
unnest_tokens(word, title) %>%
group_by(X, .add=TRUE)
